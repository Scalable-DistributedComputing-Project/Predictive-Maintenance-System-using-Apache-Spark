{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "384500a73001c022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:00:31.296488Z",
     "start_time": "2024-10-09T08:00:29.910090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py4j in /home/beboy/anaconda3/lib/python3.12/site-packages (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:00:33.567864Z",
     "start_time": "2024-10-09T08:00:33.563938Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from multiprocessing.reduction import duplicate\n",
    "from babel.util import distinct\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, isnan, when, isnull, sum as spark_sum, countDistinct\n",
    "from pyspark.sql.types import NumericType, TimestampType, DateType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import col, min, max, count, lag, datediff\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7bfc9d6cec6db19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:00:45.093966Z",
     "start_time": "2024-10-09T08:00:34.399508Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/21 15:12:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor Data Schema:\n",
      "root\n",
      " |-- equipment_id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- vibration: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- rotational_speed: double (nullable = true)\n",
      " |-- power_output: double (nullable = true)\n",
      " |-- noise_level: double (nullable = true)\n",
      " |-- voltage: double (nullable = true)\n",
      " |-- current: double (nullable = true)\n",
      " |-- oil_viscosity: double (nullable = true)\n",
      "\n",
      "Sensor Data Sample:\n",
      "+------------+--------------------+-----------+----------+---------+----------------+------------+-----------+---------+---------+-------------+\n",
      "|equipment_id|           timestamp|temperature| vibration| pressure|rotational_speed|power_output|noise_level|  voltage|  current|oil_viscosity|\n",
      "+------------+--------------------+-----------+----------+---------+----------------+------------+-----------+---------+---------+-------------+\n",
      "|           1|2024-05-21 14:54:...|   62.29391|0.38600662|102.67639|       1254.7874|    470.0774|   69.76519|223.38998|105.57472|     44.44392|\n",
      "|           1|2024-05-22 02:54:...|   62.34231|0.44370145|89.344536|        942.1218|   449.72906|   64.45927|228.45743|101.16392|     54.92587|\n",
      "|           1|2024-05-22 14:54:...|   48.20941|0.38183844|120.86258|       1031.1465|   418.68243|  63.884216|219.40497|107.26134|    54.136196|\n",
      "|           1|2024-05-23 02:54:...|   46.09723|  0.528641| 70.69653|       1056.9968|   560.51294|  75.921974| 220.7598|104.95298|    55.066605|\n",
      "|           1|2024-05-23 14:54:...|  56.484352|0.42702046| 102.9593|      1009.81573|   482.06793|   69.31188|226.03604|99.166954|    48.465614|\n",
      "+------------+--------------------+-----------+----------+---------+----------------+------------+-----------+---------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Maintenance Logs Schema:\n",
      "root\n",
      " |-- equipment_id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- maintenance_type: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- technician_id: integer (nullable = true)\n",
      " |-- duration_hours: double (nullable = true)\n",
      " |-- cost: double (nullable = true)\n",
      " |-- parts_replaced: string (nullable = true)\n",
      " |-- maintenance_result: string (nullable = true)\n",
      "\n",
      "Maintenance Logs Sample:\n",
      "+------------+--------------------+----------------+--------------------+-------------+--------------+---------+--------------+------------------+\n",
      "|equipment_id|                date|maintenance_type|         description|technician_id|duration_hours|     cost|parts_replaced|maintenance_result|\n",
      "+------------+--------------------+----------------+--------------------+-------------+--------------+---------+--------------+------------------+\n",
      "|           1|2024-09-14 14:54:...|         Routine|Routine maintenan...|            9|      7.819127|159.87044|         Seals|        Successful|\n",
      "|           2|2024-05-26 14:54:...|     Replacement|Replacement maint...|           13|     6.3528643|3776.2217|          None|        Successful|\n",
      "|           2|2024-12-12 14:54:...|         Routine|Routine maintenan...|           34|     3.5400107|  943.294|       Filters|        Successful|\n",
      "|           2|2025-02-22 14:54:...|          Repair|Repair maintenanc...|           37|     4.1757145|1885.4275|          None|           Partial|\n",
      "|           2|2025-04-14 14:54:...|         Routine|Routine maintenan...|           15|      5.206836| 4689.952|       Filters|        Successful|\n",
      "+------------+--------------------+----------------+--------------------+-------------+--------------+---------+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Equipment Specifications Schema:\n",
      "root\n",
      " |-- equipment_id: integer (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- installation_date: timestamp (nullable = true)\n",
      " |-- max_temperature: double (nullable = true)\n",
      " |-- max_pressure: double (nullable = true)\n",
      " |-- max_rotational_speed: double (nullable = true)\n",
      " |-- expected_lifetime_years: double (nullable = true)\n",
      " |-- warranty_period_years: integer (nullable = true)\n",
      " |-- last_major_overhaul: timestamp (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- criticality: string (nullable = true)\n",
      "\n",
      "Equipment Specifications Sample:\n",
      "+------------+----------+-------------+--------------------+---------------+------------+--------------------+-----------------------+---------------------+--------------------+---------+-----------+\n",
      "|equipment_id|     model| manufacturer|   installation_date|max_temperature|max_pressure|max_rotational_speed|expected_lifetime_years|warranty_period_years| last_major_overhaul| location|criticality|\n",
      "+------------+----------+-------------+--------------------+---------------+------------+--------------------+-----------------------+---------------------+--------------------+---------+-----------+\n",
      "|           1|Model-5308|ManufacturerB|2015-10-22 14:54:...|       83.05091|   188.24435|           1166.8541|              19.752157|                    1|2017-12-21 14:54:...|Section-1|     Medium|\n",
      "|           2|Model-9595|ManufacturerB|2017-09-09 14:54:...|       89.24062|   183.93924|          1007.85974|              14.115558|                    5|2020-10-10 14:54:...|Section-5|       High|\n",
      "|           3|Model-8222|ManufacturerB|2016-07-30 14:54:...|       86.23196|   170.50363|           1543.1057|              11.227789|                    5|2020-03-14 14:54:...|Section-2|     Medium|\n",
      "|           4|Model-3757|ManufacturerC|2015-08-28 14:54:...|      89.866356|   154.25136|           1394.1115|              12.238517|                    2|2017-09-29 14:54:...|Section-5|        Low|\n",
      "|           5|Model-6010|ManufacturerA|2017-03-28 14:54:...|       88.88855|   187.55278|           1253.0366|              10.239541|                    3|2018-09-20 14:54:...|Section-4|       High|\n",
      "+------------+----------+-------------+--------------------+---------------+------------+--------------------+-----------------------+---------------------+--------------------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Operational Data Schema:\n",
      "root\n",
      " |-- equipment_id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- production_rate: double (nullable = true)\n",
      " |-- operating_hours: double (nullable = true)\n",
      " |-- downtime_hours: double (nullable = true)\n",
      " |-- operator_id: integer (nullable = true)\n",
      " |-- product_type: string (nullable = true)\n",
      " |-- raw_material_quality: string (nullable = true)\n",
      " |-- ambient_temperature: double (nullable = true)\n",
      " |-- ambient_humidity: double (nullable = true)\n",
      "\n",
      "Operational Data Sample:\n",
      "+------------+--------------------+---------------+---------------+--------------+-----------+------------+--------------------+-------------------+----------------+\n",
      "|equipment_id|                date|production_rate|operating_hours|downtime_hours|operator_id|product_type|raw_material_quality|ambient_temperature|ambient_humidity|\n",
      "+------------+--------------------+---------------+---------------+--------------+-----------+------------+--------------------+-------------------+----------------+\n",
      "|           1|2024-05-21 14:54:...|      95.751816|      20.595163|     2.0935876|         68|       TypeB|                High|          30.412014|       58.801975|\n",
      "|           1|2024-05-22 14:54:...|       82.48747|      20.193354|     2.3241408|         51|       TypeA|                High|          12.546379|        41.60704|\n",
      "|           1|2024-05-23 14:54:...|       91.31933|      23.158445|     2.7285662|         31|       TypeC|                High|          28.647215|        40.01256|\n",
      "|           1|2024-05-24 14:54:...|       87.06725|      20.964285|     2.1160126|         32|       TypeC|                High|          30.238943|       42.001064|\n",
      "|           1|2024-05-25 14:54:...|       80.82978|      22.990921|     3.0622396|         15|       TypeA|                High|          21.438204|        69.99182|\n",
      "+------------+--------------------+---------------+---------------+--------------+-----------+------------+--------------------+-------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Predictive Maintenance System\").getOrCreate()\n",
    "\n",
    "# Read datasets with inferred schema\n",
    "sensor_df = spark.read.csv(\"/home/beboy/Documents/projects/scale/Predictive-Maintenance-System-using-Apache-Spark/Data Processing & Analysis/Data Generator/sensor_data.csv\", header=True, inferSchema=True)\n",
    "maintenance_df = spark.read.csv(\"/home/beboy/Documents/projects/scale/Predictive-Maintenance-System-using-Apache-Spark/Data Processing & Analysis/Data Generator/maintenance_logs.csv\", header=True, inferSchema=True)\n",
    "equipment_df = spark.read.csv(\"/home/beboy/Documents/projects/scale/Predictive-Maintenance-System-using-Apache-Spark/Data Processing & Analysis/Data Generator/equipment_specs.csv\", header=True, inferSchema=True)\n",
    "operational_df = spark.read.csv(\"/home/beboy/Documents/projects/scale/Predictive-Maintenance-System-using-Apache-Spark/Data Processing & Analysis/Data Generator/operational_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display the inferred schema and first few rows of each dataset\n",
    "print(\"Sensor Data Schema:\")\n",
    "sensor_df.printSchema()\n",
    "print(\"Sensor Data Sample:\")\n",
    "sensor_df.show(5)\n",
    "\n",
    "print(\"\\nMaintenance Logs Schema:\")\n",
    "maintenance_df.printSchema()\n",
    "print(\"Maintenance Logs Sample:\")\n",
    "maintenance_df.show(5)\n",
    "\n",
    "print(\"\\nEquipment Specifications Schema:\")\n",
    "equipment_df.printSchema()\n",
    "print(\"Equipment Specifications Sample:\")\n",
    "equipment_df.show(5)\n",
    "\n",
    "print(\"\\nOperational Data Schema:\")\n",
    "operational_df.printSchema()\n",
    "print(\"Operational Data Sample:\")\n",
    "operational_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf514d1f4ecfede",
   "metadata": {},
   "source": [
    "# Data Validation & Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903b5455d7f243f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:00:56.609551Z",
     "start_time": "2024-10-09T08:00:56.601826Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"General steps apply to all datasets\n",
    "1. Check for null values in each column\n",
    "2. Identify duplicate rows\n",
    "3. Validate data types for each column\n",
    "4. Check for consistency in equipment_id across all datasets\"\"\"\n",
    "\n",
    "def general_data_validation(df, dataset_name):\n",
    "    print(f\"General Data Validation for {dataset_name}\")\n",
    "    \n",
    "    # 1. Check for null values in each column\n",
    "    print(\"1. Null values count for each column:\")\n",
    "    null_counts = []\n",
    "    for column in df.columns:\n",
    "        # Get the data type of the column\n",
    "        data_type = df.schema[column].dataType\n",
    "        \n",
    "        # For numeric columns, check for both null and NaN\n",
    "        if isinstance(data_type, NumericType):\n",
    "            null_counts.append(count(when(col(column).isNull() | isnan(col(column)), column)).alias(column))\n",
    "        # For timestamp or date columns, only check for null\n",
    "        elif isinstance(data_type, (TimestampType, DateType)):\n",
    "            null_counts.append(count(when(col(column).isNull(), column)).alias(column))\n",
    "        # For other types (like string), only check for null\n",
    "        else:\n",
    "            null_counts.append(count(when(col(column).isNull(), column)).alias(column))\n",
    "    \n",
    "    # Use select to apply the counting operation\n",
    "    null_df = df.select(null_counts)\n",
    "    \n",
    "    print(\"Null value counts for each column:\")\n",
    "    null_df.show()\n",
    "    \n",
    "    # 2. Identify and handle duplicate rows\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.distinct().count()\n",
    "    duplicate_rows = total_rows - distinct_rows\n",
    "    print(f\"2. Duplicate rows:\")\n",
    "    print(f\"   Total rows: {total_rows}\")\n",
    "    print(f\"   Distinct rows: {distinct_rows}\")\n",
    "    print(f\"   Duplicate rows: {duplicate_rows}\")\n",
    "    \n",
    "    # 3. Validate data types for each column\n",
    "    print(\"3. Data types for each column:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # 4. Check for consistency in equipment_id across all datasets\n",
    "    if 'equipment_id' in df.columns:\n",
    "        unique_equipment_ids = df.select('equipment_id').distinct().count()\n",
    "        print(f\"4. Number of unique equipment ids in {dataset_name}: {unique_equipment_ids}\")\n",
    "    else:\n",
    "        print(f\"4. 'equipment_id' column is missing in {dataset_name}'\")\n",
    "    print(f\"End of General Data Validation for {dataset_name} \\n\")\n",
    "    \n",
    "def check_equipment_id_consistency(sensor_df, maintenance_df, equipment_df, operational_df):\n",
    "    print(\"Cross-dataset equipment_id consistency check\")\n",
    "    \n",
    "    sensor_ids = sensor_df.select('equipment_id').distinct()\n",
    "    maintenance_ids = maintenance_df.select('equipment_id').distinct()\n",
    "    equipment_ids = equipment_df.select('equipment_id').distinct()\n",
    "    operation_ids = operational_df.select('equipment_id').distinct()\n",
    "    \n",
    "    all_ids = sensor_ids.union(maintenance_ids).union(equipment_ids).union(operation_ids).distinct()\n",
    "    \n",
    "    total_unique_ids = all_ids.count()\n",
    "    print(f\"Total unique equipment_ids across all datasets: {total_unique_ids}\")\n",
    "    \n",
    "    print(\"equipment_ids not present in all datasets:\")\n",
    "    missing_ids = all_ids.join(sensor_ids, on='equipment_id', how='left_anti') \\\n",
    "                         .union(all_ids.join(maintenance_ids, on='equipment_id', how='left_anti')) \\\n",
    "                         .union(all_ids.join(equipment_ids, on='equipment_id', how='left_anti')) \\\n",
    "                         .union(all_ids.join(operation_ids, on='equipment_id', how='left_anti')) \\\n",
    "                         .distinct()\n",
    "    \n",
    "    missing_ids.show()\n",
    "    print(f\"Number of equipment_ids not consistently present: {missing_ids.count()}\")\n",
    "    print(\"End of Cross-dataset equipment_id Consistency Check\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11742c125539ca5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:01:38.415655Z",
     "start_time": "2024-10-09T08:00:57.563413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Data Validation for Sensor Data\n",
      "1. Null values count for each column:\n",
      "Null value counts for each column:\n",
      "+------------+---------+-----------+---------+--------+----------------+------------+-----------+-------+-------+-------------+\n",
      "|equipment_id|timestamp|temperature|vibration|pressure|rotational_speed|power_output|noise_level|voltage|current|oil_viscosity|\n",
      "+------------+---------+-----------+---------+--------+----------------+------------+-----------+-------+-------+-------------+\n",
      "|           0|        0|          0|        0|       0|               0|           0|          0|      0|      0|            0|\n",
      "+------------+---------+-----------+---------+--------+----------------+------------+-----------+-------+-------+-------------+\n",
      "\n",
      "2. Duplicate rows:\n",
      "   Total rows: 73000\n",
      "   Distinct rows: 73000\n",
      "   Duplicate rows: 0\n",
      "3. Data types for each column:\n",
      "root\n",
      " |-- equipment_id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- vibration: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- rotational_speed: double (nullable = true)\n",
      " |-- power_output: double (nullable = true)\n",
      " |-- noise_level: double (nullable = true)\n",
      " |-- voltage: double (nullable = true)\n",
      " |-- current: double (nullable = true)\n",
      " |-- oil_viscosity: double (nullable = true)\n",
      "\n",
      "4. Number of unique equipment ids in Sensor Data: 100\n",
      "End of General Data Validation for Sensor Data \n",
      "\n",
      "General Data Validation for Maintenance Logs\n",
      "1. Null values count for each column:\n",
      "Null value counts for each column:\n",
      "+------------+----+----------------+-----------+-------------+--------------+----+--------------+------------------+\n",
      "|equipment_id|date|maintenance_type|description|technician_id|duration_hours|cost|parts_replaced|maintenance_result|\n",
      "+------------+----+----------------+-----------+-------------+--------------+----+--------------+------------------+\n",
      "|           0|   0|               0|          0|            0|             0|   0|             0|                 0|\n",
      "+------------+----+----------------+-----------+-------------+--------------+----+--------------+------------------+\n",
      "\n",
      "2. Duplicate rows:\n",
      "   Total rows: 370\n",
      "   Distinct rows: 370\n",
      "   Duplicate rows: 0\n",
      "3. Data types for each column:\n",
      "root\n",
      " |-- equipment_id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- maintenance_type: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- technician_id: integer (nullable = true)\n",
      " |-- duration_hours: double (nullable = true)\n",
      " |-- cost: double (nullable = true)\n",
      " |-- parts_replaced: string (nullable = true)\n",
      " |-- maintenance_result: string (nullable = true)\n",
      "\n",
      "4. Number of unique equipment ids in Maintenance Logs: 98\n",
      "End of General Data Validation for Maintenance Logs \n",
      "\n",
      "General Data Validation for Equipment Specifications\n",
      "1. Null values count for each column:\n",
      "Null value counts for each column:\n",
      "+------------+-----+------------+-----------------+---------------+------------+--------------------+-----------------------+---------------------+-------------------+--------+-----------+\n",
      "|equipment_id|model|manufacturer|installation_date|max_temperature|max_pressure|max_rotational_speed|expected_lifetime_years|warranty_period_years|last_major_overhaul|location|criticality|\n",
      "+------------+-----+------------+-----------------+---------------+------------+--------------------+-----------------------+---------------------+-------------------+--------+-----------+\n",
      "|           0|    0|           0|                0|              0|           0|                   0|                      0|                    0|                  0|       0|          0|\n",
      "+------------+-----+------------+-----------------+---------------+------------+--------------------+-----------------------+---------------------+-------------------+--------+-----------+\n",
      "\n",
      "2. Duplicate rows:\n",
      "   Total rows: 100\n",
      "   Distinct rows: 100\n",
      "   Duplicate rows: 0\n",
      "3. Data types for each column:\n",
      "root\n",
      " |-- equipment_id: integer (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- installation_date: timestamp (nullable = true)\n",
      " |-- max_temperature: double (nullable = true)\n",
      " |-- max_pressure: double (nullable = true)\n",
      " |-- max_rotational_speed: double (nullable = true)\n",
      " |-- expected_lifetime_years: double (nullable = true)\n",
      " |-- warranty_period_years: integer (nullable = true)\n",
      " |-- last_major_overhaul: timestamp (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- criticality: string (nullable = true)\n",
      "\n",
      "4. Number of unique equipment ids in Equipment Specifications: 100\n",
      "End of General Data Validation for Equipment Specifications \n",
      "\n",
      "General Data Validation for Operational Data\n",
      "1. Null values count for each column:\n",
      "Null value counts for each column:\n",
      "+------------+----+---------------+---------------+--------------+-----------+------------+--------------------+-------------------+----------------+\n",
      "|equipment_id|date|production_rate|operating_hours|downtime_hours|operator_id|product_type|raw_material_quality|ambient_temperature|ambient_humidity|\n",
      "+------------+----+---------------+---------------+--------------+-----------+------------+--------------------+-------------------+----------------+\n",
      "|           0|   0|              0|              0|             0|          0|           0|                   0|                  0|               0|\n",
      "+------------+----+---------------+---------------+--------------+-----------+------------+--------------------+-------------------+----------------+\n",
      "\n",
      "2. Duplicate rows:\n",
      "   Total rows: 36500\n",
      "   Distinct rows: 36500\n",
      "   Duplicate rows: 0\n",
      "3. Data types for each column:\n",
      "root\n",
      " |-- equipment_id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- production_rate: double (nullable = true)\n",
      " |-- operating_hours: double (nullable = true)\n",
      " |-- downtime_hours: double (nullable = true)\n",
      " |-- operator_id: integer (nullable = true)\n",
      " |-- product_type: string (nullable = true)\n",
      " |-- raw_material_quality: string (nullable = true)\n",
      " |-- ambient_temperature: double (nullable = true)\n",
      " |-- ambient_humidity: double (nullable = true)\n",
      "\n",
      "4. Number of unique equipment ids in Operational Data: 100\n",
      "End of General Data Validation for Operational Data \n",
      "\n",
      "Cross-dataset equipment_id consistency check\n",
      "Total unique equipment_ids across all datasets: 100\n",
      "equipment_ids not present in all datasets:\n",
      "+------------+\n",
      "|equipment_id|\n",
      "+------------+\n",
      "|          83|\n",
      "|          89|\n",
      "+------------+\n",
      "\n",
      "Number of equipment_ids not consistently present: 2\n",
      "End of Cross-dataset equipment_id Consistency Check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/21 15:13:08 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "sensor_df_clean = general_data_validation(sensor_df, \"Sensor Data\")\n",
    "maintenance_df_clean = general_data_validation(maintenance_df, \"Maintenance Logs\")\n",
    "equipment_df_clean = general_data_validation(equipment_df, \"Equipment Specifications\")\n",
    "operational_df_clean = general_data_validation(operational_df, \"Operational Data\")\n",
    "check_equipment_id_consistency(sensor_df, maintenance_df, equipment_df, operational_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
